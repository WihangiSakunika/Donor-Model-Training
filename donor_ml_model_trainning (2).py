# -*- coding: utf-8 -*-
"""Donor_ML_Model_Trainning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o_1mrVWiB5Xt3ESF6Y4QgnZ2o5VS3tSF

Installation
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import seaborn as sns

"""# Load the data | Data Exploration"""

import pandas as pd

# Load the data - make sure the file path is correct
data = pd.read_csv('donor_data.csv')

# Now you can use the data variable
print(data.head())
print(data.info())
print(data['TARGET_B'].value_counts())

"""# Data Preprocessing - Handle missing values"""

data = data.replace('?', np.nan)
data = data.replace(' ', np.nan)

# Drop columns with too many missing values or not useful
drop_cols = ['CONTROL_NUMBER', 'TARGET_D']  # TARGET_D has many missing values
data = data.drop(columns=drop_cols, errors='ignore')

# Convert categorical variables
cat_cols = ['URBANICITY', 'HOME_OWNER', 'DONOR_GENDER', 'PUBLISHED_PHONE',
            'OVERLAY_SOURCE', 'CLUSTER_CODE', 'SES', 'RECENCY_STATUS_96NK']
for col in cat_cols:
    if col in data.columns:
        data[col] = LabelEncoder().fit_transform(data[col].astype(str))

# Fill missing values
for col in data.columns:
    if data[col].dtype == 'object':
        data[col] = data[col].fillna(data[col].mode()[0])
    else:
        data[col] = data[col].fillna(data[col].median())

"""### Feature Engineering"""

# Create a new target for generous donors (top 20% of donors by LIFETIME_GIFT_AMOUNT)
data['GENEROUS_DONOR'] = 0
donors = data[data['TARGET_B'] == 1]
threshold = donors['LIFETIME_GIFT_AMOUNT'].quantile(0.8)
data.loc[(data['TARGET_B'] == 1) & (data['LIFETIME_GIFT_AMOUNT'] >= threshold), 'GENEROUS_DONOR'] = 1

# Prepare features and targets
X = data.drop(columns=['TARGET_B', 'GENEROUS_DONOR'])
y_donation = data['TARGET_B']  # For predicting who will donate
y_generous = data['GENEROUS_DONOR']  # For predicting generous donors among donors

# Split data
X_train, X_test, y_train_d, y_test_d = train_test_split(X, y_donation, test_size=0.2, random_state=42)
_, _, y_train_g, y_test_g = train_test_split(X, y_generous, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Handle class imbalance with SMOTE
smote = SMOTE(random_state=42)
X_train_smote_d, y_train_smote_d = smote.fit_resample(X_train_scaled, y_train_d)
X_train_smote_g, y_train_smote_g = smote.fit_resample(X_train_scaled, y_train_g)

"""## Model 1: Random Forest for Donation Prediction"""

# Train-test split for donation prediction
X_train_don, X_test_don, y_train_don, y_test_don = train_test_split(
    X, y_donation, test_size=0.2, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_don)
X_test_scaled = scaler.transform(X_test_don)

# Apply SMOTE
X_train_smote_d, y_train_smote_d = smote.fit_resample(X_train_scaled, y_train_don)

# Tuned Random Forest for Donation Prediction
rf_donation = RandomForestClassifier(
    n_estimators=800,
    max_depth=30,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features="sqrt",
    class_weight="balanced",
    random_state=42,
    n_jobs=-1
)

rf_donation.fit(X_train_smote_d, y_train_smote_d)
y_pred_rf_d = rf_donation.predict(X_test_scaled)

print("\nTuned Random Forest Donation Prediction Results:")
print(f"Accuracy: {accuracy_score(y_test_don, y_pred_rf_d):.2f}")
print(classification_report(y_test_don, y_pred_rf_d))

"""## Model 2: Deep Neural Network for Donation Prediction"""

model_donation = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_smote_d.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model_donation.compile(optimizer=Adam(learning_rate=0.001),
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
history_d = model_donation.fit(X_train_smote_d, y_train_smote_d,
                             epochs=50,
                             batch_size=32,
                             validation_split=0.2,
                             callbacks=[early_stop],
                             verbose=1)

# Evaluate NN
y_pred_nn_d = (model_donation.predict(X_test_scaled) > 0.5).astype(int)
print("\nNeural Network Donation Prediction Results:")
print(f"Accuracy: {accuracy_score(y_test_d, y_pred_nn_d):.2f}")
print(classification_report(y_test_d, y_pred_nn_d))

"""## Model 3: Predicting Generous Donors (only on donors)"""

donor_data = data[data['TARGET_B'] == 1]
X_donor = donor_data.drop(columns=['TARGET_B', 'GENEROUS_DONOR'])
y_donor = donor_data['GENEROUS_DONOR']

X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_donor, y_donor, test_size=0.2, random_state=42)
X_train_d_scaled = scaler.fit_transform(X_train_d)
X_test_d_scaled = scaler.transform(X_test_d)

# SMOTE for generous donor prediction
X_train_smote_gd, y_train_smote_gd = smote.fit_resample(X_train_d_scaled, y_train_d)

# Random Forest for generous donors
rf_generous = RandomForestClassifier(n_estimators=100, random_state=42)
rf_generous.fit(X_train_smote_gd, y_train_smote_gd)
y_pred_rf_g = rf_generous.predict(X_test_d_scaled)

print("\nRandom Forest Generous Donor Prediction Results:")
print(f"Accuracy: {accuracy_score(y_test_d, y_pred_rf_g):.2f}")
print(classification_report(y_test_d, y_pred_rf_g))

# Feature Importance for generous donors
feature_imp_g = pd.Series(rf_generous.feature_importances_, index=X_donor.columns).sort_values(ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_imp_g[:10], y=feature_imp_g.index[:10])
plt.title('Top 10 Important Features for Generous Donor Prediction')
plt.show()

"""## Model 4: Neural Network for Generous Donors"""

model_generous = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_smote_gd.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model_generous.compile(optimizer=Adam(learning_rate=0.001),
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

history_g = model_generous.fit(X_train_smote_gd, y_train_smote_gd,
                             epochs=50,
                             batch_size=32,
                             validation_split=0.2,
                             callbacks=[early_stop],
                             verbose=1)
# Evaluate NN for generous donors
y_pred_nn_g = (model_generous.predict(X_test_d_scaled) > 0.5).astype(int)
print("\nNeural Network Generous Donor Prediction Results:")
print(f"Accuracy: {accuracy_score(y_test_d, y_pred_nn_g):.2f}")
print(classification_report(y_test_d, y_pred_nn_g))

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Assuming you have:
# X - features
# y_donation - binary target for donation prediction (0/1)
# y_generous - binary target for generosity prediction (0/1)

# 1. Split data for donation model
X_train_donation, X_test_donation, y_train_donation, y_test_donation = train_test_split(
    X, y_donation, test_size=0.2, random_state=42)

# 2. Split data for generosity model (different split)
X_train_generous, X_test_generous, y_train_generous, y_test_generous = train_test_split(
    X, y_generous, test_size=0.2, random_state=42)

# 3. Define preprocessing
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# 4. Create and train donation model
rf_donation = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])
rf_donation.fit(X_train_donation, y_train_donation)

# 5. Create and train generosity model
rf_generous = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])
rf_generous.fit(X_train_generous, y_train_generous)

# 6. Evaluate models
y_pred_donation = rf_donation.predict(X_test_donation)
print("Donation Prediction Accuracy:", accuracy_score(y_test_donation, y_pred_donation))

y_pred_generous = rf_generous.predict(X_test_generous)
print("Generosity Prediction Accuracy:", accuracy_score(y_test_generous, y_pred_generous))

"""1. Who doesnâ€™t donate (Give donate / Non-donor recognition)"""

non_donors = data[data['TARGET_B'] == 0]
print("Total Non-Donors:", len(non_donors))

"""2. Who donates the most of those who donate"""

top_donor = data[data['TARGET_B'] == 1].sort_values(by="LIFETIME_GIFT_AMOUNT", ascending=False).head(1)
print(top_donor[["LIFETIME_GIFT_AMOUNT"]])

"""3. 85% accuracy requirement

4. Maximum donate person & Average donation
"""

max_donation = data[data['TARGET_B'] == 1]['LIFETIME_GIFT_AMOUNT'].max()
avg_donation = data[data['TARGET_B'] == 1]['LIFETIME_GIFT_AMOUNT'].mean()
print("Maximum Donation:", max_donation)
print("Average Donation:", avg_donation)

"""5. Recognized Donor and Non-Donor

6. Dataset enough or not
"""

print(data['TARGET_B'].value_counts(normalize=True))

"""7. Final ML Training Result"""

print("=== Final Results ===")

# Donation prediction results
print("Random Forest Donation Accuracy:", accuracy_score(y_test_donation, y_pred_rf_d))
print("Neural Network Donation Accuracy:", accuracy_score(y_test_donation, y_pred_nn_d))

# Generous donor prediction results (only donors subset)
print("Random Forest Generous Donor Accuracy:", accuracy_score(y_test_d, y_pred_rf_g))
print("Neural Network Generous Donor Accuracy:", accuracy_score(y_test_d, y_pred_nn_g))

from IPython.display import display
import pandas as pd

# 1. Donors and Non-donors
non_donors = data[data['TARGET_B'] == 0]
donors = data[data['TARGET_B'] == 1]

# 2. Top Donor
top_donor = donors.loc[donors['LIFETIME_GIFT_AMOUNT'].idxmax()]

# 3. Accuracy check
rf_acc = accuracy_score(y_test_donation, y_pred_rf_d)
nn_acc = accuracy_score(y_test_donation, y_pred_nn_d)
accuracy_status = "Met" if (rf_acc >= 0.85 or nn_acc >= 0.85) else " Not Met"

# 4. Donation stats
max_donation = donors['LIFETIME_GIFT_AMOUNT'].max()
avg_donation = donors['LIFETIME_GIFT_AMOUNT'].mean()

# 5. Donor vs Non-donor counts
donor_count = len(donors)
non_donor_count = len(non_donors)

# 6. Dataset enough or not
donor_ratio = donor_count / len(data)
dataset_status = " Imbalanced (needs SMOTE/more donor data)" if donor_ratio < 0.1 else " Reasonable"

# Put all results into a DataFrame
results = pd.DataFrame({
    "Question": [
        "1. Who doesnâ€™t donate (Non-donors)?",
        "2. Who donates the most?",
        "3. 85% Accuracy requirement?",
        "4. Maximum & Average Donation",
        "5. Recognized Donor & Non-Donor counts",
        "6. Dataset enough or not?"
    ],
    "Answer": [
        f"Non-Donors: {non_donor_count}, Donors: {donor_count}",
        f"Top Donor ID: {top_donor.name}, Amount: {top_donor['LIFETIME_GIFT_AMOUNT']}",
        f"RF: {rf_acc:.2%}, NN: {nn_acc:.2%} â†’ {accuracy_status}",
        f"Max: {max_donation:.2f}, Avg: {avg_donation:.2f}",
        f"Donors: {donor_count}, Non-Donors: {non_donor_count}",
        f"Donor Ratio: {donor_ratio:.2%} â†’ {dataset_status}"
    ]
})

display(results)